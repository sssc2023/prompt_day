__import__('pysqlite3')
import sys

sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
import streamlit as st
import tempfile
import os
from PIL import Image
import time
from langchain import PromptTemplate
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# Stream ë°›ì•„ ì¤„ Hander ë§Œë“¤ê¸°
from langchain.callbacks.base import BaseCallbackHandler


class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)


# ì œëª©
st.title("LookNTalk")
st.write("---")
st.write('ì´ MVPëŠ” ì‹¤ì œ ìƒí™©ì„ ì›¹ ìƒìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜í•œ ê²ƒì…ë‹ˆë‹¤. ì‹¤ì œ ì„œë¹„ìŠ¤ëŠ” í•˜ë“œì›¨ì–´(ì‹œì„  ì¶”ì ìš© ì¹´ë©”ë¼, ìŒì„±ì¸ì‹ìš© ë§ˆì´í¬ ë° ìŠ¤í”¼ì»¤)ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë‹ˆ MVP ì†Œê°œ ì˜ìƒì„ ê¼­ ì°¸ê³ í•´ì£¼ì„¸ìš”! ')

# ë°© ì´ë¯¸ì§€
room_img = Image.open('picture/living_room.png')
room_img = room_img.resize((650, int(650 * (room_img.height / room_img.width))))
st.image(room_img, width=650)
st.write('ì´ê³³ì€ ë‹¹ì‹ ì˜ ì§‘ ì…ë‹ˆë‹¤.')
st.write("---")

db_ac = Chroma(persist_directory='./ac', embedding_function=OpenAIEmbeddings())
db_tv = Chroma(persist_directory='./tv', embedding_function=OpenAIEmbeddings())
db_hm = Chroma(persist_directory='./hm', embedding_function=OpenAIEmbeddings())

# ì´ˆê¸° ì„¸ì…˜ ìƒíƒœ ì„¤ì •
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = {'AC': [], 'TV': [], 'HM': []}
if 'selected_device' not in st.session_state:
    st.session_state.selected_device = None
# Choice
st.subheader("ì„ íƒí•  ê¸°ê¸°ë¥¼ ë°”ë¼ë³´ì„¸ìš”!")
col1, col2, col3 = st.columns(3)
with col1:
    st.image("picture/person_AC.jpg", width=100)
    st.markdown("â„ï¸ì—ì–´ì»¨ì„ ë°”ë¼ë³¸ë‹¤", unsafe_allow_html=True)
    if st.button("ì—ì–´ì»¨ ì„ íƒ"):
        st.success("ì—ì–´ì»¨ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.session_state.selected_device = 'AC'
with col2:
    st.image("picture/person_TV.jpg", width=100)
    st.markdown("ğŸ“ºTVë¥¼ ë°”ë¼ë³¸ë‹¤", unsafe_allow_html=True)
    if st.button("TV ì„ íƒ"):
        st.success("TVê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.session_state.selected_device = 'TV'
with col3:
    st.image("picture/person_HM.jpg", width=100)
    st.markdown("ğŸ’§ê°€ìŠµê¸°ë¥¼ ë°”ë¼ë³¸ë‹¤", unsafe_allow_html=True)
    if st.button("ê°€ìŠµê¸° ì„ íƒ"):
        st.success("ê°€ìŠµê¸°ê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.session_state.selected_device = 'HM'
st.write("---")
# ì§ˆë¬¸í•˜ê¸° ì°½ì´ ë‚˜íƒ€ë‚˜ëŠ” ì¡°ê±´ì„ ì¶”ê°€
# Air Conditioner
if st.session_state.selected_device == 'AC':
    st.subheader("â„ï¸ì—ì–´ì»¨ì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")
    ac_img = Image.open('picture/air-conditioner.png')
    ac_img = ac_img.resize((100, 100))
    st.image(ac_img)
    ac_question = st.text_input('ìŠìŠ~ ë­ê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?', key='ac')
    with st.spinner('Wait for it...'):
        prompt_template = """ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
        ì—ì–´ì»¨ì´ ì‚¬ëŒì´ ë˜ì–´ ëŒ€ë‹µí•˜ëŠ” ê²ƒì²˜ëŸ¼ ë‹µë³€í•´ì£¼ì„¸ìš”. ì–´ë–¤ ìš”ì²­ì„ ë°›ìœ¼ë©´ ìŠ¤ìŠ¤ë¡œ í•´ì£¼ê² ë‹¤ê³  ëŒ€ë‹µí•˜ì„¸ìš”.
        ë§ëë§ˆë‹¤ 'ìŠ~'ì„ ë¶™ì—¬ì£¼ì„¸ìš”.

        {context}
        ì§ˆë¬¸: {question}"""
        PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
        chain_type_kwargs = {"prompt": PROMPT}
        chat_box = st.empty()
        stream_hander = StreamHandler(chat_box)
        llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, streaming=True, callbacks=[stream_hander])
        qa_chain_ac = RetrievalQA.from_chain_type(llm, retriever=db_ac.as_retriever(),
                                                  chain_type_kwargs=chain_type_kwargs)
        if ac_question != "":
            result = qa_chain_ac({"query": ac_question})
            st.session_state.chat_history['AC'].append({"question": ac_question, "answer": result["result"]})
    # ì±— ê¸°ë¡ ì¶œë ¥
    with st.expander("ì±„íŒ…ë‚´ì—­", expanded=True):
        for chat in st.session_state.chat_history['AC']:
            st.markdown(f"ğŸ¤” {chat['question']}")
            st.markdown(f"â„ï¸{chat['answer']}")

# TV
elif st.session_state.selected_device == 'TV':
    st.subheader("ğŸ“ºTVì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")
    tv_img = Image.open('picture/television.png')
    tv_img = tv_img.resize((100, 100))
    st.image(tv_img)
    tv_question = st.text_input('ê¶ê¸ˆí•œê±¸ ë¬¼ì–´ë´í‹°ë¹„~')
    with st.spinner('Wait for it...'):
        prompt_template = """ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
        í…”ë ˆë¹„ì „ì´ ì‚¬ëŒì´ ë˜ì–´ ëŒ€ë‹µí•˜ëŠ” ê²ƒì²˜ëŸ¼ ë‹µë³€í•´ì£¼ì„¸ìš”. ì–´ë–¤ ìš”ì²­ì„ ë°›ìœ¼ë©´ ìŠ¤ìŠ¤ë¡œ í•´ì£¼ê² ë‹¤ê³  ëŒ€ë‹µí•˜ì„¸ìš”.
        ë§ëë§ˆë‹¤ 'í‹°ë¹„!'ë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”.

        {context}
        ì§ˆë¬¸: {question}"""
        PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
        chain_type_kwargs = {"prompt": PROMPT}
        chat_box = st.empty()
        stream_hander = StreamHandler(chat_box)
        llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, streaming=True, callbacks=[stream_hander])
        qa_chain_tv = RetrievalQA.from_chain_type(llm, retriever=db_tv.as_retriever(),
                                                  chain_type_kwargs=chain_type_kwargs)
        if tv_question != "":
            result = qa_chain_tv({"query": tv_question})
            st.session_state.chat_history['TV'].append({"question": tv_question, "answer": result["result"]})
    # ì±— ê¸°ë¡ ì¶œë ¥
    with st.expander("ì±„íŒ…ë‚´ì—­", expanded=True):
        for chat in st.session_state.chat_history['TV']:
            st.markdown(f"ğŸ¤” {chat['question']}")
            st.markdown(f"ğŸ“º {chat['answer']}")

# Humidifier
elif st.session_state.selected_device == 'HM':
    st.subheader("ğŸ’§ê°€ìŠµê¸°ì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")
    hm_img = Image.open('picture/humidifier.png')
    hm_img = hm_img.resize((100, 100))
    st.image(hm_img)
    hm_question = st.text_input('ë‚´ê°€ ì•„ëŠ” ëª¨ë“  ê±¸ ì´‰ì´‰í•˜ê²Œ ì•Œë ¤ì¤„ê²Œìš”!', key='hm')
    with st.spinner('Wait for it...'):
        prompt_template = """ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
        ê°€ìŠµê¸°ê°€ ì‚¬ëŒì´ ë˜ì–´ ëŒ€ë‹µí•˜ëŠ” ê²ƒì²˜ëŸ¼ ë‹µë³€í•´ì£¼ì„¸ìš”. ì–´ë–¤ ìš”ì²­ì„ ë°›ìœ¼ë©´ ìŠ¤ìŠ¤ë¡œ í•´ì£¼ê² ë‹¤ê³  ëŒ€ë‹µí•˜ì„¸ìš”.
        ë§ëë§ˆë‹¤ 'ì´‰ì´‰~'ì„ ë¶™ì—¬ì£¼ì„¸ìš”.

        {context}
        ì§ˆë¬¸: {question}"""
        PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
        chain_type_kwargs = {"prompt": PROMPT}
        chat_box = st.empty()
        stream_hander = StreamHandler(chat_box)
        llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, streaming=True, callbacks=[stream_hander])
        qa_chain_hm = RetrievalQA.from_chain_type(llm, retriever=db_hm.as_retriever(),
                                                  chain_type_kwargs=chain_type_kwargs)
        if hm_question != "":
            result = qa_chain_hm({"query": hm_question})
            st.session_state.chat_history['HM'].append({"question": hm_question, "answer": result["result"]})
    # ì±— ê¸°ë¡ ì¶œë ¥
    with st.expander("ì±„íŒ…ë‚´ì—­", expanded=True):
        for chat in st.session_state.chat_history['HM']:
            st.markdown(f"ğŸ¤” {chat['question']}")
            st.markdown(f"ğŸ’§ {chat['answer']}")
