__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
import streamlit as st
import tempfile
import os
from PIL import Image
import time

#íŒŒì¼ ì—…ë¡œë“œ
# ["samsung_tv_manual.pdf", "lg_ac_manual.pdf", "winix_humidifier_manual.pdf"]
tv_file = PyPDFLoader("samsung_tv_manual.pdf")
ac_file = PyPDFLoader("lg_ac_manual.pdf")
hm_file = PyPDFLoader("winix_humidifier_manual.pdf")

#ì œëª©
st.title("SightnSpeak")
st.write("---")

# ë°© ì´ë¯¸ì§€
cyworld_img = Image.open('livingroom.jpg')
# ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
cyworld_img = cyworld_img.resize((650, int(650 * (cyworld_img.height / cyworld_img.width))))
st.image(cyworld_img, width=650)
st.write("---")

def document_to_db(uploaded_file, size):    # ë¬¸ì„œ í¬ê¸°ì— ë§ê²Œ ì‚¬ì´ì¦ˆ ì§€ì •í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ì•„ì„œ para ë„£ì—ˆì–´ìš©
    pages = uploaded_file.load_and_split()
    #Split
    text_splitter = RecursiveCharacterTextSplitter(
        # Set a really small chunk size, just to show.
        chunk_size = size,
        chunk_overlap  = 20,
        length_function = len,
        is_separator_regex = False,
    )
    texts = text_splitter.split_documents(pages)

    #Embedding
    embeddings_model = OpenAIEmbeddings()

    # load it into Chroma
    db = Chroma.from_documents(texts, embeddings_model)
    return db

def wrap_text(text, line_length=16): # ì±—ë´‡ ê¸€ììˆ˜ ì¡°ì ˆ..
    lines = []
    for i in range(0, len(text), line_length):
        lines.append(text[i:i + line_length])
    return "\n".join(lines)


# ì´ˆê¸° ì„¸ì…˜ ìƒíƒœ ì„¤ì •
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = {'AC': [], 'TV': [], 'HM': []}

# ì—…ë¡œë“œ ë˜ë©´ ë™ì‘í•˜ëŠ” ì½”ë“œ

db_ac = document_to_db(ac_file, 500)
db_tv = document_to_db(tv_file, 500)
db_hm = document_to_db(hm_file, 300)

# Choice
st.subheader("ê¸°ê¸°ë¥¼ ë°”ë¼ë³´ê³  ì„ íƒí•˜ì„¸ìš”!")
col1, col2, col3 = st.columns(3)
st.subheader("PDFì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")
col_ac, col_tv, col_hm = st.columns(3)

with col1:
    st.image("person_AC.jpg", width=100)
    st.markdown("ì—ì–´ì»¨ì„ <br/> ë°”ë¼ë³¸ë‹¤", unsafe_allow_html=True)
    if st.button("ì—ì–´ì»¨ ì„ íƒ"):
        st.write("ì—ì–´ì»¨ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
        with col_ac:
            st.subheader("ì—ì–´ì»¨")
            ac_img = Image.open('air-conditioner.png')
            ac_img = ac_img.resize((100, 100))
            st.image(ac_img)
            ac_question = st.text_input('ì—ì–´ì»¨ì—ê²Œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”', key='ac')
            if st.button('ì—ì–´ì»¨ì—ê²Œ ì§ˆë¬¸í•˜ê¸°'):
                with st.spinner('Wait for it...'):
                    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
                    qa_chain = RetrievalQA.from_chain_type(llm, retriever=db_ac.as_retriever())
                    result = qa_chain({"query": ac_question})
                    st.session_state.chat_history['AC'].append({"question": ac_question, "answer": result["result"]})

            # ì±— ê¸°ë¡ ì¶œë ¥
            for chat in st.session_state.chat_history['AC']:
                st.text(f"ğŸ¤” {wrap_text(chat['question'])}")
                st.text(f"ğŸ˜Š {wrap_text(chat['answer'])}")
                st.write("---")

with col2:
    st.image("person_TV.jpg", width=100)
    st.markdown("TVë¥¼ <br/> ë°”ë¼ë³¸ë‹¤", unsafe_allow_html=True)
    if st.button("TV ì„ íƒ"):
        st.write("TVê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
        with col_tv:
            st.subheader("TV")
            tv_img = Image.open('television.png')
            tv_img = tv_img.resize((100, 100))
            st.image(tv_img)
            tv_question = st.text_input('TVì—ê²Œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”')
            if st.button('TVì—ê²Œ ì§ˆë¬¸í•˜ê¸°', key='tv_button'):
                with st.spinner('Wait for it...'):
                    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
                    qa_chain = RetrievalQA.from_chain_type(llm, retriever=db_tv.as_retriever())
                    result = qa_chain({"query": tv_question})
                    st.session_state.chat_history['TV'].append({"question": tv_question, "answer": result["result"]})

            # ì±— ê¸°ë¡ ì¶œë ¥
            for chat in st.session_state.chat_history['TV']:
                st.text(f"ğŸ¤” {wrap_text(chat['question'])}")
                st.text(f"ğŸ˜Š {wrap_text(chat['answer'])}")
                st.write("---")

with col3:
    st.image("person_HM.jpg", width=100)
    st.markdown("ê°€ìŠµê¸°ë¥¼ <br/> ë°”ë¼ë³¸ë‹¤", unsafe_allow_html=True)
    if st.button("ê°€ìŠµê¸° ì„ íƒ"):
        st.write("ê°€ìŠµê¸°ê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
        with col_hm:
            st.subheader("ê°€ìŠµê¸°")
            hm_img = Image.open('humidifier.png')
            hm_img = hm_img.resize((100, 100))
            st.image(hm_img)
            hm_question = st.text_input('ê°€ìŠµê¸°ì—ê²Œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”', key='hm')
            if st.button('ê°€ìŠµê¸°ì—ê²Œ ì§ˆë¬¸í•˜ê¸°'):
                with st.spinner('Wait for it...'):
                    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
                    qa_chain = RetrievalQA.from_chain_type(llm, retriever=db_hm.as_retriever())
                    result = qa_chain({"query": hm_question})
                    st.session_state.chat_history['HM'].append({"question": hm_question, "answer": result["result"]})

            # ì±— ê¸°ë¡ ì¶œë ¥
            for chat in st.session_state.chat_history['HM']:
                st.text(f"ğŸ¤” {wrap_text(chat['question'])}")
                st.text(f"ğŸ˜Š {wrap_text(chat['answer'])}")
                st.write("---")
